---
layout: post
title: Agents
author: [Richard Kuo]
category: [Lecture]
tags: [jekyll, ai]
---

Introduction to AI Agents, Langchain, SWE.

---
## Agents
<iframe width="576" height="325" src="https://www.youtube.com/embed/bJZTJ7MjYqg?list=PLJV_el3uVTsPz6CTopeRp2L2t4aL_KgiI" title="【生成式AI導論 2024】第9講：以大型語言模型打造的AI Agent (14:50 教你怎麼打造芙莉蓮一級魔法使考試中出現的泥人哥列姆)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### [LLM Agent Paper List](https://github.com/WooooDyy/LLM-Agent-Paper-List)
**Paper:** [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/abs/2309.07864)<br>
![](https://github.com/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure1.jpg)

---
### Survey of LLM-based AI Agents
**Paper:** [An In-depth Survey of Large Language Model-based Artificial Intelligence Agents](https://arxiv.org/abs/2309.14365)<br>
![](https://github.com/rkuo2000/AI-course/blob/main/images/AI_agent_Overview_of_the_planning_component.png?raw=true)
![](https://github.com/rkuo2000/AI-course/blob/main/images/AI_agent_Mapping_Structure_of_Memory.png?raw=true)

**Blog:** [4 Autonomous AI Agents you need to know](https://towardsdatascience.com/4-autonomous-ai-agents-you-need-to-know-d612a643fa92)<br>

---
### Camel
Communicative Agents for “Mind” Exploration of Large Language Model Society<br>
**Code:** [https://github.com/camel-ai/camel](https://github.com/camel-ai/camel)
![](https://raw.githubusercontent.com/camel-ai/camel/master/misc/framework.png)

---
### AutoGPT
**Paper:** [Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions](https://arxiv.org/abs/2306.02224)<br>
**Code:** [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)<br>
**Blog:** [AutoGPT architecture & breakdown](https://www.georgesung.com/ai/autogpt-arch/)<br>
![](https://www.georgesung.com/assets/img/auto_gpt.svg)

**Tutorials:** [AutoGPT Forge](https://aiedge.medium.com/autogpt-forge-e3de53cc58ec)<br>
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*9fDToDTOEc3tzMSDIJ-Tng.png)

---
### [AgentGPT](https://agentgpt.reworkd.ai/)
**Code:** [https://github.com/reworkd/AgentGPT](https://github.com/reworkd/AgentGPT)<br>

---
### [BabyAGI](https://github.com/yoheinakajima/babyagi)
**Blog:** [Task-driven Autonomous Agent Utilizing GPT-4, Pinecone, and LangChain for Diverse Applications](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/)
![](https://github.com/rkuo2000/AI-course/blob/main/images/BabyAI_flowchart.png?raw=true)

**Colab:**<br>
* [baby_agi.ipynb](https://github.com/langchain-ai/langchain/blob/master/cookbook/baby_agi.ipynb)
* [baby_agi_with_agent.ipynb](https://github.com/langchain-ai/langchain/blob/master/cookbook/baby_agi_with_agent.ipynb)

---
### [Godmode](https://godmode.space/?ref=futuretools.io)

---
### Voyager
**Paper:** [Voyager: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/abs/2305.16291)<br>
**Code:** [https://github.com/MineDojo/Voyager](https://github.com/MineDojo/Voyager)<br>
![](https://github.com/MineDojo/Voyager/raw/main/images/pull.png)

---
### Talk2Drive
**Paper:** [Large Language Models for Autonomous Driving: Real-World Experiments](https://arxiv.org/abs/2312.09397)<br>
![](https://cdn.bytez.com/mobilePapers/v2/arxiv/2312.09397/images/2-0.png)
<iframe width="1273" height="716" src="https://www.youtube.com/embed/KlJ_dHuEkwI" title="Talk2Drive Intersection Demo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### MemGPT
**Paper:** [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560)<br>
**Code:** [https://github.com/cpacker/MemGPT](https://github.com/cpacker/MemGPT)<br>
![](https://github.com/rkuo2000/AI-course/blob/main/images/MemGPT.png?raw=true)

---
### [RL-GPT](https://sites.google.com/view/rl-gpt/)
**Paper:** [RL-GPT: Integrating Reinforcement Learning and Code-as-policy](https://arxiv.org/abs/2402.19299)<br>
![](https://lh4.googleusercontent.com/MaBwcpR6hX2GAUJZuiPE4_E60Xuf1lCievhp4LDfgYj0mV9wpQYE7yQKh1ekaqLKtt0YDlVltM6Ng_qGL2YhNHI0dQbXxaJObVTNMpps6T5wuz6WHgRY9SaDJeUdylCl0w=w1280)

---
### MC-Planner
**Paper:** [https://arxiv.org/abs/2302.01560](https://arxiv.org/abs/2302.01560)<br>
**Code:** [https://github.com/CraftJarvis/MC-Planner](https://github.com/CraftJarvis/MC-Planner)<br>
![](https://github.com/rkuo2000/AI-course/blob/main/images/Minecraft_planning.png?raw=true)

---
## Multi-Agent

### Generative Agents
**Paper:** [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)<<br>
**Code:** [https://github.com/joonspk-research/generative_agents](https://github.com/joonspk-research/generative_agents)<br>
**[Demo](https://reverie.herokuapp.com/arXiv_Demo/#)**<br>
![](https://github.com/joonspk-research/generative_agents/raw/main/cover.png)

**Blog:** [Paper Review: Generative Agents: Interactive Simulacra of Human Behavior](https://artgor.medium.com/paper-review-generative-agents-interactive-simulacra-of-human-behavior-cc5f8294b4ac)<br>
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*e-o-Iz3WumTLI994IRQTuw.jpeg)

<iframe width="1165" height="655" src="https://www.youtube.com/embed/G44Lkj7XDsA" title="【生成式AI】讓 AI 村民組成虛擬村莊會發生甚麼事？" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### Multi-Agent Collaboration
![](https://dl-staging-website.ghost.io/content/images/size/w1000/2024/04/unnamed---2024-04-17T155856.845-1.png)
* [“Communicative Agents for Software Development,” Qian et al. (2023) (the ChatDev paper)](https://arxiv.org/abs/2307.07924)
* [“AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,” Wu et al. (2023)](https://arxiv.org/abs/2308.08155)
* [“MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework,” Hong et al. (2023)](https://arxiv.org/abs/2308.00352)

---
### Multi-Agent examples
* [multi-agent-collaboration.ipynb](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi-agent-collaboration.ipynb)
![](https://raw.githubusercontent.com/langchain-ai/langgraph/97dc410b08696efab30850868f44a56b1032a78c/examples/multi_agent/img/simple_multi_agent_diagram.png)

* [hierarchical_agent_teams.ipynb](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb)
![](https://raw.githubusercontent.com/langchain-ai/langgraph/97dc410b08696efab30850868f44a56b1032a78c/examples/multi_agent/img/hierarchical-diagram.png)

* [agent_supervisor.ipynb](https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb)
![](https://raw.githubusercontent.com/langchain-ai/langgraph/97dc410b08696efab30850868f44a56b1032a78c/examples/multi_agent/img/supervisor-diagram.png)

---
### LangGraph + Llama3 + Groq
**Colab:** [https://drp.li/X3hpZ](https://drp.li/X3hpZ)<br>
<iframe width="515" height="290" src="https://www.youtube.com/embed/lvQ96Ssesfk" title="Creating an AI Agent with LangGraph Llama 3 &amp; Groq" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
## Frameworks

### [LangChain](https://github.com/langchain-ai/langchain)
* [LangChain documents](https://js.langchain.com/docs/get_started/introduction)
* [LangChain use-cases](https://js.langchain.com/docs/use_cases)
* [LangChain cookbook](https://github.com/langchain-ai/langchain/tree/master/cookbook)

---
### [LangGraph](https://langchain-ai.github.io/langgraph/)
**[Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)**<br>
![](https://github.com/rkuo2000/AI-course/blob/main/images/LangGraph_intro.png?raw=true)<br>

---
## SWE

### OpenDevin
**Paper:** [SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING](https://swe-agent.com/paper.pdf)<br>
**Code:** [https://github.com/OpenDevin/OpenDevin](https://github.com/OpenDevin/OpenDevin)<br>
**Docs:** [OpenDevin Intro](https://opendevin.github.io/OpenDevin/modules/usage/intro)<br>
![](https://github.com/OpenDevin/OpenDevin/raw/main/docs/static/img/screenshot.png)
![](https://github.com/OpenDevin/OpenDevin/assets/38853559/92b622e3-72ad-4a61-8f41-8c040b6d5fb3)

---
### DroidAgent
**Paper:** [Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing](https://arxiv.org/abs/2311.08649)<br>
**Code:** [DroidAgent: Intent-Driven Android GUI Testing with LLM Agents](https://github.com/coinse/droidagent)<br>
![](https://github.com/coinse/droidagent/raw/main/resources/droidagent.jpg)

---
### WebVoyager
**Paper:** [WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models](https://arxiv.org/abs/2401.13919)<br>
**Code:** [https://github.com/MinorJerry/WebVoyager](https://github.com/MinorJerry/WebVoyager)<br>
![](https://raw.githubusercontent.com/MinorJerry/WebVoyager/main/assets/overall_process_crop.png)

---
### AutoCodeRover
**Paper:** [AutoCodeRover: Autonomous Program Improvement](https://arxiv.org/html/2404.05427v1)<br>
![](https://arxiv.org/html/2404.05427v1/x1.png)
![](https://arxiv.org/html/2404.05427v1/x2.png)

---
## ReAct

### [Chain of thought and ReAct](https://abvijaykumar.medium.com/prompt-engineering-chain-of-thought-and-react-sql-agent-85fa42575c06)
<p><img width="50%" height="50%" src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*XBh0aKnnFvI5wvpi5LAv4A.png"></p>
1. **Thought**: The reasoning step, or thought, serves as a guide to the Foundation model, demonstrating how to approach a problem. It involves formulating a sequence of questions that lead the model to the desired solution.<br>
2. **Action**: Once the thought is established, the next step is to define an action for the Foundation model to take. This action typically involves invoking an API from a predefined set, allowing the model to interact with external resources.<br>
3. **Observation**: Following the action, the model observes and analyzes the results. The observations become crucial input for further reasoning and decision-making.<br>

---
#### List chain-of-thought steps:
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*lB2NwFbn7vgMoR1h50IB4g.png)

---
#### List ReAct steps:
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*sbCsRX6D5PI9wbF8r8K3-w.png)

---
### [Langchain ReAct](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/react/)

---
### Llamaindex ReAct
[ReAct Agent - A Simple Intro with Calculator Tools](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/)<br>
[ReAct Agent with Query Engine (RAG) Tools](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent_with_query_engine/)<br>
[Controlling Agent Reasoning Loop with Return Direct Tools](https://docs.llamaindex.ai/en/stable/examples/agent/return_direct_agent/)<br>

[Fine-tuning a gpt-3.5 ReAct Agent on Better Chain of Thought](https://docs.llamaindex.ai/en/stable/examples/finetuning/react_agent/react_agent_finetune/)
[Custom Cohere Reranker](https://docs.llamaindex.ai/en/stable/examples/finetuning/rerankers/cohere_custom_reranker/)<br>

---
### [Octpus](https://choiszt.github.io/Octopus/)
**Paper:** [Octopus: Embodied Vision-Language Programmer from Environmental Feedback](https://arxiv.org/abs/2310.08588)<br>
**Code:** [https://github.com/dongyh20/Octopus](https://github.com/dongyh20/Octopus)<br>

### [Octpus v2](https://arxiv.org/html/2404.01744)
**Paper:** [Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744)<br>
**model:** [NexaAIDev/Octopus-v2](https://huggingface.co/NexaAIDev/Octopus-v2)<br>
![](https://huggingface.co/NexaAIDev/Octopus-v2/resolve/main/tool-usage-compressed.png)
**kaggle:** [https://www.kaggle.com/code/rkuo2000/octopus-v2](https://www.kaggle.com/code/rkuo2000/octopus-v2)<br>
<iframe width="942" height="530" src="https://www.youtube.com/embed/jhM0D0OObOw" title="Nexa AI&#39;s Super Agent LLM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### [Octpus v3](https://arxiv.org/html/2404.11459v2)
**Paper:** [Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent](https://arxiv.org/abs/2404.11459)<br>

---
### Octpus v4
**Paper:** [Octopus v4: Graph of language models](https://arxiv.org/abs/2404.19296)<br>
**model:** [NexaAIDev/octo-net](https://huggingface.co/NexaAIDev/octo-net)<br>
**Code:** [https://github.com/NexaAI/octopus-v4](https://github.com/NexaAI/octopus-v4)<br>

---
### ADAS
**Paper:** [Automated Design of Agentic Systems](https://arxiv.org/abs/2408.08435)<br>
**Code:** [https://github.com/ShengranHu/ADAS](https://github.com/ShengranHu/ADAS)<br>
![](https://github.com/ShengranHu/ADAS/raw/main/misc/algo.png)

---
### [Gödel Agent](https://arxiv.org/html/2410.04444v1)
**Paper:** [Gödel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement](https://arxiv.org/abs/2410.04444)<br>
**Code:** [https://github.com/semanser/codel](https://github.com/semanser/codel)<br>
![](https://arxiv.org/html/2410.04444v1/x1.png)
語言模型隨著代理系統(agentic system)的發展，在推理、工作規畫等領域有很大幅度的進步，這些代理系統(agentic system)主要可以分為兩大類，一是固定整個工作流與工作模組的Hand-Designed Agent；另外一種則是允許較彈性的工作流，並讓Agent可以適度選用工具的Meta-Learning Optimized Agents。但這兩者皆是基於人類先驗經驗而設計的系統，它將受限於人類的經驗，使得整個系統失去最佳化的可能性。<br>
本篇論文的研究團隊，嘗試利用哥德爾機(Gödel machine)的概念，讓Agent可以自行決定工作流程，自行選用工具模組，並依照環境反饋，自我改良整個工作系統，研究團隊將其稱為Gödel Agent。<br>

在這篇概念性的論文中，研究團隊指出如果要能實現自我優化，Gödel Agent至少需具備四種能力：<br>
1. **自我覺察(Self-Awareness)**
能夠讀取工作流當下，環境與Agent的各式變數、函式、類等參數值，取得整體的運作狀態(operating state)。<br>
2. **自我改善(Self-Improvement)**
3. 能夠利用推理與規劃的能力，針對當下狀況，認知到應該調整那些工作區塊，並進而調整程式碼去修改工作邏輯。
4. **與環境互動(Environmental Interaction)**
針對當前修改的結果，可以由環境的狀態變化取得反饋，得知目前的策略是否成功，並評估是否需要再度調整。<br>
5. **持續改進(Recursive Improvement)**
利用前三項能力，不停迭代，在經過幾次迭代後，這將產生類似Gödel machine的效果，以達到整個系統的最佳化。<br>

研究團隊給予Gödel Agent幾種不同的工作類型，測試它的表現，並與過往幾種方法，諸如CoT、Self-Refine、Role Assignment、Meta Agent Search進行比較，就結果上來說Gödel Agent完勝，不過由於測試的工作類型較侷限，目前尚未知道Gödel Agent在不同的領域是否都如此出色。<br>
研究團隊也給出未來的研究方向，例如語言模型能否產生集體智慧(collective Intelligence)，或者Gödel Agent是否確實達到系統理論上的最佳化(theoretical optimality)等，都是有趣的研究主題。<br>

---
### [OpenAI Swarm](https://github.com/openai/swarm)
![](https://github.com/openai/swarm/raw/main/assets/swarm_diagram.png)

#### llama3-groq
```
import openai
from google.colab import userdata

model = "llama3-groq-70b-8192-tool-use-preview"

llm_client = openai.OpenAI(
  base_url="https://api.groq.com/openai/v1",
  api_key=userdata.get('GROQ_API_KEY'),
)
```

#### bare_minimum
```
# https://github.com/openai/swarm/blob/main/examples/basic/bare_minimum.py
from swarm import Swarm, Agent

swarm_client = Swarm(client=llm_client)

agent = Agent(
    name="Agent",
    instructions="You are a helpful agent.",
    model=model,
    tool_choice="auto"
)

messages = [{"role": "user", "content": "Hi!"}]
response = swarm_client.run(agent=agent, messages=messages)

print(response.messages[-1]["content"])
```

#### [Swarm_Llama3-Groq.ipynb](https://colab.research.google.com/github/sbagency/AI-agents-hacks/blob/main/Openai_swarm_Llama3_Groq_ipynb%22.ipynb)

---
### MLE-bench
**Paper:** [MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering](https://arxiv.org/abs/2410.07095)<br>
**Code:** [https://github.com/openai/mle-bench](https://github.com/openai/mle-bench)<br>
![](https://yblog.org/wp-content/uploads/2024/10/202410aio.webp)
![](https://www.marktechpost.com/wp-content/uploads/2024/10/Screenshot-2024-10-12-at-11.39.51-AM.png)

---
### AI Agent分析中華隊 12 強預賽成績
[AI Agent：實戰篇 — 透過 Google Gemini Model 和 Langchain 來分析中華隊 12 強預賽成績](https://medium.com/@simon3458/google-gemini-model-langchain-ai-agent-example-9ed797179a88)<br>

---
### [TEN Agent](https://agent.theten.ai/)

<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*
