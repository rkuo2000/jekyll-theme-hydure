---
layout: post
title: Retrieval-Augmented Generation
author: [Richard Kuo]
category: [Lecture]
tags: [jekyll, ai]
---

Introduction to RAG, LlamaIndex, examples.

---
## RAG (檢索增強生成)
**blog:** [RAG 檢索增強生成— 讓大型語言模型更聰明的秘密武器](https://blog.infuseai.io/rag-retrieval-augmented-generation-introduction-a5854cb6393e)<br>
![](https://blogs.mathworks.com/deep-learning/files/2024/01/rag.png)

---
### RAG Survey
**Paper:** [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)<br>
**Paper:** [Retrieval-Augmented Generation for AI-Generated Content: A Survey](https://arxiv.org/abs/2402.19473)<br>
**Paper:** [A Survey on Retrieval-Augmented Text Generation for Large Language Models](https://arxiv.org/abs/2404.10981)<br>
**Paper:** [RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing](https://arxiv.org/abs/2404.19543)<br>
**Paper:** [A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2405.06211)<br>
**Paper:** [Evaluation of Retrieval-Augmented Generation: A Survey](https://arxiv.org/abs/2405.07437)<br>
**Paper:** [Retrieval-Augmented Generation for Natural Language Processing: A Survey](https://arxiv.org/abs/2407.13193)<br>
**Paper:** [Graph Retrieval-Augmented Generation: A Survey](https://arxiv.org/abs/2408.08921)<br>
**Paper:** [RAG and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely](https://arxiv.org/abs/2409.14924)<br>
**Paper:** [A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions](https://arxiv.org/abs/2410.12837)<br>

---
#### [A Guide on 12 Tuning Strategies for Production-Ready RAG Applications](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439#156e)
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*tT14GpYfEMSqCjnt2UQOGQ.png)

---
#### [NLP • Retrieval Augmented Generation](https://aman.ai/primers/ai/RAG/)
<p><img width="50%" height="50%" src="https://aman.ai/primers/ai/assets/RAG/4.png"></p>

---
### BGE (BAAI General Embedding)
**Code:** [https://github.com/FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)<br>

---
### EAGLE-LLM
3X faster for LLM<br>
**Blog:** [EAGLE: Lossless Acceleration of LLM Decoding by Feature Extrapolation](https://sites.google.com/view/eagle-llm)<br>
**Code:** [https://github.com/SafeAILab/EAGLE](https://github.com/SafeAILab/EAGLE)<br>
**Kaggle:** [https://www.kaggle.com/code/rkuo2000/eagle-llm](https://www.kaggle.com/code/rkuo2000/eagle-llm)<br>
![](https://lh5.googleusercontent.com/DmWpgZKMdJigadi-THqozK36pQ4MQuXx0jR3GugvXrd9qNPztsOjPl1OO42MdyN3MyLkh9GvgWTVu61cDWd3MsQf57lRG6kEhZxSe2m24uPUKQ33zHDNaKQb659w5kFaLQ=w1280)

---
### PurpleLlama CyberSecEval
**Paper:** [PurpleLlama CyberSecEval: A Secure Coding Benchmark for Language Models](https://arxiv.org/abs/2312.04724)<br>
**Code:** [CybersecurityBenchmarks](https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks)<br>
[meta-llama/LlamaGuard-7b](https://huggingface.co/meta-llama/LlamaGuard-7b)<br>
<table>
<tr><th>           </th><th>Our Test Set (Prompt)</th><th>OpenAI Mod</th><th>ToxicChat</th><th>Our Test Set (Response)</th></tr>
<tr><td>Llama-Guard</td><td>0.945</td><td>0.847</td><td>0.626</td><td>0.953</td></tr>
<tr><td>OpenAI API</td><td>	0.764</td><td>0.856</td><td>0.588</td><td>0.769</td></tr>
<tr><td>Perspective API</td><td>0.728</td><td>0.787</td><td>0.532</td><td>0.699</td></tr>
</table>

---
### [Contextual Retrieval RAG](https://www.anthropic.com/news/contextual-retrieval)
[Anthropic Prompt Caching](https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb)<br>

#### Standard RAG
![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F45603646e979c62349ce27744a940abf30200d57-3840x2160.png&w=3840&q=75)
#### Contextual Retrieval RAG
![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2496e7c6fedd7ffaa043895c23a4089638b0c21b-3840x2160.png&w=3840&q=75)
![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8f82c6175a64442ceff4334b54fac2ab3436a1d1-3840x2160.png&w=3840&q=75)

**Blog:** [Implementing Contextual Retrieval in RAG pipeline](https://medium.com/the-ai-forum/implementing-contextual-retrieval-in-rag-pipeline-8f1bc7cbd5e0)<br>

---
## Frameworks

### [LlamaIndex](https://www.llamaindex.ai/)
**Code:** [https://github.com/run-llama/llama_index](https://github.com/run-llama/llama_index)<br>
**Docs:** [](https://docs.llamaindex.ai/en/stable/)<br>
![](https://docs.llamaindex.ai/en/stable/_static/getting_started/basic_rag.png)

---
### [LangChain](https://www.langchain.com/)
* **langchain-core**: Base abstractions and LangChain Expression Language.
* Integration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.
* **langchain**: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.
* **langchain-community**: Third-party integrations that are community maintained.
* **LangGraph**: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.
* **LangGraph Platform**: Deploy LLM applications built with LangGraph to production.
* [LangSmith](https://docs.smith.langchain.com/): A developer platform that lets you debug, test, evaluate, and monitor LLM applications.

**Kaggle:** [https://www.kaggle.com/code/rkuo2000/rag-with-langchain](https://www.kaggle.com/code/rkuo2000/rag-with-langchain)<br>

---
### [RAG with MATLAB](https://blogs.mathworks.com/deep-learning/2024/01/22/large-language-models-with-matlab/)
![](https://blogs.mathworks.com/deep-learning/files/2024/01/LLMs_recording.gif)

---
### [Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)
**[https://github.com/ray-project/llm-applications/blob/main/notebooks/rag.ipynb](https://github.com/ray-project/llm-applications/blob/main/notebooks/rag.ipynb)**<br>
(1)將外部文件做分塊(chunking)再分詞(tokenize)轉成token<br>
(2)利用嵌入模型，將token做嵌入(embeds)運算，轉成向量，儲存至向量資料庫(Vector Database)並索引(Indexes)<br>
(3)用戶提出問題，向量資料庫將問題字串轉成向量(利用前一個步驟的嵌入模型)，再透過餘弦(Cosine)相似度或歐氏距離演算法來搜尋資料庫裡的近似資料<br>
(4)將用戶的問題、資料庫查詢結果一起放進Prompt(提示)，交由LLM推理出最終答案<br>
以上是基本的RAG流程，利用Langchain或LlamaIndex或Haystack之類的應用程式開發框架，大概用不到一百行的程式碼就能做掉(含LLM的裝載)。<br>

Anyscale剛剛發布的一篇精彩好文，裡頭介紹了很多提升RAG成效的高段技巧，內容包括：<br>
🚀從頭開始建構基於RAG的LLM應用程式。<br>
🚀 在具有不同運算資源的多個工作人員之間擴展主要工作負載（載入、分塊、嵌入、索引、服務等）。<br>
🚀評估應用程式的不同配置，以最佳化每個元件（例如retrieval_score）和整體效能（quality_score）。<br>
🚀 透過開源和閉源LLM實作混合代理路由方法，以建立效能最佳且最具成本效益的應用程式。<br>
🚀以高擴展性與高可用性的方式為應用程式提供服務。<br>
🚀了解微調、提示工程、詞彙搜尋(lexical search)、重新排名、資料飛輪(data flywheel)等方法如何影響應用程式的效能。<br>

---
### CRAG
**Paper:** [Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884)<br>
**Blog:** [CRAG: 檢索增強生成的糾錯機制 - 提升大型語言模型問答精確度](https://ithelp.ithome.com.tw/articles/10348884)<br>
**Code:** [https://github.com/HuskyInSalt/CRAG](https://github.com/HuskyInSalt/CRAG)<br>

---
### GRAG
**Paper:** [From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/pdf/2404.16130)<br>
**Paper:** [GRAG: Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2405.16506)<br>
**Blog:** [GraphRAG: Unlocking LLM discovery on narrative private data](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)<br>
**Blog:** [Knowledge Graph + RAG | Microsoft GraphRAG 實作與視覺化教學](https://medium.com/@cch.chichieh/knowledge-graph-rag-microsoft-graphrag-%E5%AF%A6%E4%BD%9C%E8%88%87%E8%A6%96%E8%A6%BA%E5%8C%96%E6%95%99%E5%AD%B8-ac07991855e6)<br>
**Code:** [https://github.com/microsoft/graphrag](https://github.com/microsoft/graphrag)<br>
**Code:** [GraphRAG Accelerator](https://github.com/Azure-Samples/graphrag-accelerator)<br>

---
### HippoRAG
**Paper:** [HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models](https://arxiv.org/abs/2405.14831)<br>
**Code:** [https://github.com/OSU-NLP-Group/HippoRAG](https://github.com/OSU-NLP-Group/HippoRAG)<br>
![](https://github.com/OSU-NLP-Group/HippoRAG/raw/main/images/hook_figure.png)

---
### TAG (Table-Augmented Generation)
**Paper:** [Text2SQL is Not Enough: Unifying AI and Databases with TAG](https://arxiv.org/abs/2408.14717)<br>
**Blog:** [Goodbye, Text2SQL: Why Table-Augmented Generation (TAG) is the Future of AI-Driven Data Queries!](https://ai.plainenglish.io/goodbye-text2sql-why-table-augmented-generation-tag-is-the-future-of-ai-driven-data-queries-892e24e06922)<br>
**Code:** [https://github.com/TAG-Research/TAG-Bench](https://github.com/TAG-Research/TAG-Bench)<br>
![](https://github.com/TAG-Research/TAG-Bench/raw/main/assets/tag.png)

---
### Groq(Llava) + Qwen2-VL 
**Blog:** [Implement Multimodal RAG with ColPali and Vision Language Model Groq(Llava) and Qwen2-VL](https://medium.com/the-ai-forum/implement-multimodal-rag-with-colpali-and-vision-language-model-groq-llava-and-qwen2-vl-5c113b8c08fd)<br>
![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*zTJq5b3EKPk_Qq29.png)

---
### MemoRAG
**paper:** [MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery
](https://arxiv.org/abs/2409.05591)<br>
**Code:** [https://github.com/qhjqhj00/MemoRAG](https://github.com/qhjqhj00/MemoRAG)<br>
![](https://github.com/qhjqhj00/MemoRAG/raw/main/asset/tech_case.jpg)

<iframe width="677" height="366" src="https://www.youtube.com/embed/Nr3XWcGzeSg" title="&quot;I want RAG to remember everything about my private knowledge locally&quot; - Here&#39;s How." frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### VisRAG
**Paper:** [VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents](https://arxiv.org/abs/2410.10594)<br>
**Code:** [https://github.com/openbmb/visrag](https://github.com/openbmb/visrag)<br>
![](https://github.com/OpenBMB/VisRAG/raw/master/assets/main_figure.png)

<iframe width="680" height="382" src="https://www.youtube.com/embed/WcLSFRoo9_A" title="AI Innovations and Trends 02: VisRAG, GraphRAG, RAGLAB, and More!" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### RAG-FiT
**Code:** [https://github.com/IntelLabs/RAG-FiT](https://github.com/IntelLabs/RAG-FiT)<br>
RAG-FiT is a library designed to improve LLMs ability to use external information by fine-tuning models on specially created RAG-augmented datasets. 
**應用實例：**<br>
* 醫學問答機器人：自動查最新論文回答
* 法律顧問AI：即時檢索法條庫
* 新聞生成器：抓時事數據寫報導
* 論文助手：整理相關研究生成文獻回顧

---
### DeepRAG
**Paper:** [DeepRAG: Thinking to Retrieval Step by Step for Large Language Models](https://arxiv.org/abs/2502.01142)<br>

![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*66RmL7YSQaGlAQsxGqTNnQ.jpeg)

**DeepRAG Construct Instruction**:<br>
```
Instruction: You are a helpful Retrieve-Augmented
Generation (RAG) model. Your task is to answer
questions by logically decomposing them into clear
sub-questions and iteratively addressing each one.
Use "Follow up:" to introduce each sub-question and
"Intermediate answer:" to provide answers.
For each sub-question, decide whether you can provide a direct answer or if additional information is
required. If additional information is needed, state,
"Let’s search the question in Wikipedia." and then use
the retrieved information to respond comprehensively.
If a direct answer is possible, provide it immediately
without searching.
```

<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*

