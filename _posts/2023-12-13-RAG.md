---
layout: post
title: Retrieval Augmented Generation (RAG)
author: [Richard Kuo]
category: [Lecture]
tags: [jekyll, ai]
---

Retrieval Augmented Generation (RAG)

---
## RAG

### [Contemporary LLMs](https://www.kaggle.com/code/rkuo2000/contemporary-large-language-models-llms)

---
### [Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2005.11401)
![](https://eugeneyan.com/assets/rag.jpg)

---
#### [A Guide on 12 Tuning Strategies for Production-Ready RAG Applications](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439#156e)
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*tT14GpYfEMSqCjnt2UQOGQ.png)

---
#### [NLP â€¢ Retrieval Augmented Generation](https://aman.ai/primers/ai/RAG/)
![](https://aman.ai/primers/ai/assets/RAG/4.png)

---
### [Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/)
![](https://eugeneyan.com/assets/llm-patterns-og.png)

---
### [Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)
![](https://images.ctfassets.net/xjan103pcp94/4PX0l1ruKqfH17YvUiMFPw/c60a7a665125cb8056bebcc146c23b76/image8.png)
(1)å°‡å¤–éƒ¨æ–‡ä»¶åšåˆ†å¡Š(chunking)å†åˆ†è©(tokenize)è½‰æˆtoken<br>
(2)åˆ©ç”¨åµŒå…¥æ¨¡å‹ï¼Œå°‡tokenåšåµŒå…¥(embeds)é‹ç®—ï¼Œè½‰æˆå‘é‡ï¼Œå„²å­˜è‡³å‘é‡è³‡æ–™åº«(Vector Database)ä¸¦ç´¢å¼•(Indexes)<br>
(3)ç”¨æˆ¶æå‡ºå•é¡Œï¼Œå‘é‡è³‡æ–™åº«å°‡å•é¡Œå­—ä¸²è½‰æˆå‘é‡(åˆ©ç”¨å‰ä¸€å€‹æ­¥é©Ÿçš„åµŒå…¥æ¨¡å‹)ï¼Œå†é€éé¤˜å¼¦(Cosine)ç›¸ä¼¼åº¦æˆ–æ­æ°è·é›¢æ¼”ç®—æ³•ä¾†æœå°‹è³‡æ–™åº«è£¡çš„è¿‘ä¼¼è³‡æ–™<br>
(4)å°‡ç”¨æˆ¶çš„å•é¡Œã€è³‡æ–™åº«æŸ¥è©¢çµæœä¸€èµ·æ”¾é€²Prompt(æç¤º)ï¼Œäº¤ç”±LLMæ¨ç†å‡ºæœ€çµ‚ç­”æ¡ˆ<br>
ä»¥ä¸Šæ˜¯åŸºæœ¬çš„RAGæµç¨‹ï¼Œåˆ©ç”¨Langchainæˆ–LlamaIndexæˆ–Haystackä¹‹é¡çš„æ‡‰ç”¨ç¨‹å¼é–‹ç™¼æ¡†æ¶ï¼Œå¤§æ¦‚ç”¨ä¸åˆ°ä¸€ç™¾è¡Œçš„ç¨‹å¼ç¢¼å°±èƒ½åšæ‰(å«LLMçš„è£è¼‰)ã€‚<br>

Anyscaleå‰›å‰›ç™¼å¸ƒçš„ä¸€ç¯‡ç²¾å½©å¥½æ–‡ï¼Œè£¡é ­ä»‹ç´¹äº†å¾ˆå¤šæå‡RAGæˆæ•ˆçš„é«˜æ®µæŠ€å·§ï¼Œå…§å®¹åŒ…æ‹¬ï¼š<br>
ğŸš€å¾é ­é–‹å§‹å»ºæ§‹åŸºæ–¼RAGçš„LLMæ‡‰ç”¨ç¨‹å¼ã€‚<br>
ğŸš€ åœ¨å…·æœ‰ä¸åŒé‹ç®—è³‡æºçš„å¤šå€‹å·¥ä½œäººå“¡ä¹‹é–“æ“´å±•ä¸»è¦å·¥ä½œè² è¼‰ï¼ˆè¼‰å…¥ã€åˆ†å¡Šã€åµŒå…¥ã€ç´¢å¼•ã€æœå‹™ç­‰ï¼‰ã€‚<br>
ğŸš€è©•ä¼°æ‡‰ç”¨ç¨‹å¼çš„ä¸åŒé…ç½®ï¼Œä»¥æœ€ä½³åŒ–æ¯å€‹å…ƒä»¶ï¼ˆä¾‹å¦‚retrieval_scoreï¼‰å’Œæ•´é«”æ•ˆèƒ½ï¼ˆquality_scoreï¼‰ã€‚<br>
ğŸš€ é€éé–‹æºå’Œé–‰æºLLMå¯¦ä½œæ··åˆä»£ç†è·¯ç”±æ–¹æ³•ï¼Œä»¥å»ºç«‹æ•ˆèƒ½æœ€ä½³ä¸”æœ€å…·æˆæœ¬æ•ˆç›Šçš„æ‡‰ç”¨ç¨‹å¼ã€‚<br>
ğŸš€ä»¥é«˜æ“´å±•æ€§èˆ‡é«˜å¯ç”¨æ€§çš„æ–¹å¼ç‚ºæ‡‰ç”¨ç¨‹å¼æä¾›æœå‹™ã€‚<br>
ğŸš€äº†è§£å¾®èª¿ã€æç¤ºå·¥ç¨‹ã€è©å½™æœå°‹(lexical search)ã€é‡æ–°æ’åã€è³‡æ–™é£›è¼ª(data flywheel)ç­‰æ–¹æ³•å¦‚ä½•å½±éŸ¿æ‡‰ç”¨ç¨‹å¼çš„æ•ˆèƒ½ã€‚<br>

---
#### [Fusion-in-Decoder (FiD)](https://arxiv.org/abs/2007.01282)
![](https://eugeneyan.com/assets/fid.jpg)

---
#### [Retrieval-Enhanced Transformer (RETRO)](https://arxiv.org/abs/2112.04426)
![](https://eugeneyan.com/assets/retro.jpg)

---
#### [Internet-augmented LMs](https://arxiv.org/abs/2203.05115)
![](https://eugeneyan.com/assets/internet-llm.jpg)

---
#### [Overview of RAG for CodeT5+](https://arxiv.org/abs/2305.07922)
![](https://eugeneyan.com/assets/codet5.jpg)

---
#### [Hypothetical document embeddings (HyDE)](https://arxiv.org/abs/2212.10496)
![](https://eugeneyan.com/assets/hyde.jpg)

### LlamaIndex
**Code:** [https://github.com/run-llama/llama_index](https://github.com/run-llama/llama_index)<br>
**Kaggle:** [https://www.kaggle.com/code/rkuo2000/llm-llamaindex](https://www.kaggle.com/code/rkuo2000/llm-llamaindex)<br>
LlamaIndex (GPT Index) is a data framework for your LLM application.
* Offers data connectors to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)
* Provides ways to structure your data (indices, graphs) so that this data can be easily used with LLMs.
* Provides an advanced retrieval/query interface over your data: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.
* Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, ChatGPT, anything else).

---
### LLM Embedder
**Paper:** [Retrieve Anything To Augment Large Language Models](https://arxiv.org/abs/2310.07554)<br>
**Code:** [https://github.com/FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)<br>
**Kaggle:** [https://www.kaggle.com/code/rkuo2000/llm-flagembedding](https://www.kaggle.com/code/rkuo2000/llm-flagembedding)<br>
![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a4e4265-7dab-4c5d-b14f-5dfd1b270e75_746x735.png)
![](https://github.com/FlagOpen/FlagEmbedding/raw/master/FlagEmbedding/llm_embedder/imgs/llm-embedder.png)

---
### LM-Cocktail
**Paper:** [LM-Cocktail: Resilient Tuning of Language Models via Model Merging](https://arxiv.org/abs/2311.13534)<br>
**Code:** [https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)<br>

---
### EAGLE-LLM
3X faster for LLM<br>
**Blog:** [EAGLE: Lossless Acceleration of LLM Decoding by Feature Extrapolation](https://sites.google.com/view/eagle-llm)<br>
**Code:** [https://github.com/SafeAILab/EAGLE](https://github.com/SafeAILab/EAGLE)<br>
**Kaggle:** [https://www.kaggle.com/code/rkuo2000/eagle-llm](https://www.kaggle.com/code/rkuo2000/eagle-llm)<br>

---
### Purple Llama CyberSecEval
**Paper:** [Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models](https://arxiv.org/abs/2312.04724)<br>
**Code:** [CybersecurityBenchmarks](https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks)<br>
[meta-llama/LlamaGuard-7b](https://huggingface.co/meta-llama/LlamaGuard-7b)<br>
<table>
<tr><th>           </th><th>Our Test Set (Prompt)</th><th>OpenAI Mod</th><th>ToxicChat</th><th>Our Test Set (Response)</th></tr>
<tr><td>Llama-Guard</td><td>0.945</td><td>0.847</td><td>0.626</td><td>0.953</td></tr>
<tr><td>OpenAI API</td><td>	0.764</td><td>0.856</td><td>0.588</td><td>0.769</td></tr>
<tr><td>Perspective API</td><td>0.728</td><td>0.787</td><td>0.532</td><td>0.699</td></tr>
</table>

---
### Fine-tuning : To get better at specific tasks

#### [ULMFit](https://arxiv.org/abs/1801.06146)
![](https://eugeneyan.com/assets/ulmfit.jpg)

---
#### [Bidirectional Encoder Representations from Transformers (BERT; encoder only)](https://arxiv.org/abs/1810.04805)
![](https://eugeneyan.com/assets/bert.jpg)

---
#### [Generative Pre-trained Transformers (GPT; decoder only)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
![](https://eugeneyan.com/assets/gpt.jpg)

---
#### [Text-to-text Transfer Transformer (T5; encoder-decoder)](https://arxiv.org/abs/1910.10683)
![](https://eugeneyan.com/assets/t5.jpg)

---
#### [InstructGPT](https://arxiv.org/abs/2203.02155)
![](https://eugeneyan.com/assets/instructgpt.jpg)

---
#### [Soft prompt tuning](https://arxiv.org/abs/2104.08691)
**Blog:** [Guiding Frozen Language Models with Learned Soft Prompts](https://blog.research.google/2022/02/guiding-frozen-language-models-with.html)<br>
![](https://blogger.googleusercontent.com/img/a/AVvXsEgWPnqNhC2ZtEjkumYCtNi18nHLQY9U5dmV13cJzQzscVhcHYhLdpTdTv-1ZI3IaOVfWE9x7y4g75jtyImEaI7dsonfD43S24flWsevDgEdbA0oR5w6fJsnFecnKGysSguLKJKEQ5svS-aQn_ClNZm6jURazpAxFNWTQoTm708a4hFq8f2HzMVpz3wZ_g=w640-h360)
![](https://blogger.googleusercontent.com/img/a/AVvXsEgNi-pteVLIEZ6H5HdV8RadrzCkegKA3zJCM2ObwTHKKYhgF7b-c7qsN85P1j4nXcqHcIDTj2dU5KfslYU4PuIFXaDpF6o_e5jMfFWljd6Kpc0E1n-UG6LtMA5B_BIAKjWTUibhwCnQ2zWap9BiZgA-VB0bxQG-S1jMcUHZ01kl0uLIKIoqKYH8QtUiYA=s693)

---
#### [prefix tuning](https://arxiv.org/abs/2101.00190)
![](https://eugeneyan.com/assets/prefix.jpg)

---
#### [adapter](https://arxiv.org/abs/1902.00751)
![](https://eugeneyan.com/assets/adapter.jpg)

---
#### [Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685)
![](https://eugeneyan.com/assets/lora.jpg)

---
#### [QLoRA](https://arxiv.org/abs/2305.14314)
![](https://eugeneyan.com/assets/qlora.jpg)

---
### LongLoRA
**Code:** [https://github.com/dvlab-research/LongLoRA](https://github.com/dvlab-research/LongLoRA)<br>
[2023.11.19] We release a new version of LongAlpaca models, LongAlpaca-7B-16k, LongAlpaca-7B-16k, and LongAlpaca-7B-16k. <br>
![](https://github.com/dvlab-research/LongLoRA/raw/main/imgs/LongAlpaca.png)

---
## Prompt Engineering
[Prompt Engineering Guide](https://www.promptingguide.ai/)<br>

---
### Chain of Density (CoD)
**Paper:** [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269)<br>

---
### Chain of Thougths (CoT)
**Paper:** [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)<br>
**Paper:** [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)<br>
**Blog:** [Chain-of-Thought Prompting](https://www.promptingguide.ai/techniques/cot)
![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1080&q=75)
![](https://user-images.githubusercontent.com/22279212/194787183-a1f8dff8-a0ad-43a1-827f-819671503860.png)

---
### Tree of Thoughts (ToT)
**Paper:** [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)<br>
**Code:** [https://github.com/princeton-nlp/tree-of-thought-llm](https://github.com/princeton-nlp/tree-of-thought-llm)<br>
![](https://github.com/princeton-nlp/tree-of-thought-llm/blob/master/pics/teaser.png?raw=true)

---
### Soft-prompt Tuning
**Paper:** [Soft-prompt Tuning for Large Language Models to Evaluate Bias](https://arxiv.org/abs/2306.04735)<br>

---
### LLM Lingua
**Paper: [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/abs/2310.05736)<br>
**Code: [https://github.com/microsoft/LLMLingua](https://github.com/microsoft/LLMLingua)<br>
**Kaggle:** [https://www.kaggle.com/code/rkuo2000/llm-lingua](https://www.kaggle.com/code/rkuo2000/llm-lingua)<br>
![](https://github.com/microsoft/LLMLingua/raw/main/images/LLMLingua.png)

---
### Caching: To reduce latency and cost

#### [GPTCache](https://github.com/zilliztech/GPTCache)
![](https://eugeneyan.com/assets/gptcache.jpg)

---
### [Open-LLMs](https://github.com/eugeneyan/open-llms)
Open LLMs<br>
Open LLM for Coder<br>

---
## Deployment

### [LLaMA-2-7B Benchmark](https://github.com/liltom-eth/llama2-webui/blob/main/docs/performance.md)

---
### [Run Llama 2 Locally in 7 Lines! (Apple Silicon Mac)](https://blog.lastmileai.dev/run-llama-2-locally-in-7-lines-apple-silicon-mac-c3f46143f327)
![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*81Zzsz8opkq8eBUbpRHlng.png)
On an `M2 Max MacBook Pro`, I was able to get 35â€“40 tokens per second using the LLAMA_METAL build flag.<br>

---
### [Lamini LLM Finetuning on AMD ROCmâ„¢: A Technical Recipe](https://www.lamini.ai/blog/lamini-llm-finetuning-on-amd-rocm-a-technical-recipe)

---
### localLLM 
**Code:** [https://github.com/ykhli/local-ai-stack](https://github.com/ykhli/local-ai-stack)<br>
ğŸ¦™ Inference: Ollama<br>
ğŸ’» VectorDB: Supabase pgvector<br>
ğŸ§  LLM Orchestration: Langchain.js<br>
ğŸ–¼ï¸ App logic: Next.js<br>
ğŸ§® Embeddings generation: Transformer.js and all-MiniLM-L6-v2<br>

---
### AirLLM
**Blog:** [Unbelievable! Run 70B LLM Inference on a Single 4GB GPU with This NEW Technique](https://ai.gopubby.com/unbelievable-run-70b-llm-inference-on-a-single-4gb-gpu-with-this-new-technique-93e2057c7eeb)<br>
**Code:** [https://github.com/lyogavin/Anima/tree/main/air_llm](https://github.com/lyogavin/Anima/tree/main/air_llm)<br>

---
### PowerInfer
**Paper:** [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](https://arxiv.org/abs/2312.12456)<br>
**Code:** [https://github.com/SJTU-IPADS/PowerInfer](https://github.com/SJTU-IPADS/PowerInfer)<br>
**Blog:** [2080 Tiå°±èƒ½è·‘70Bå¤§æ¨¡å‹ï¼Œä¸Šäº¤å¤§æ–°æ¡†æ¶è®©LLMæ¨ç†å¢é€Ÿ11å€](https://mp.weixin.qq.com/s/GnEK3xE5EhR5N9Mzs3tOtA)<br>

https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/fe441a42-5fce-448b-a3e5-ea4abb43ba23
PowerInfer v.s. llama.cpp on a single RTX 4090(24G) running Falcon(ReLU)-40B-FP16 with a 11x speedup!
<sub>Both PowerInfer and llama.cpp were running on the same hardware and fully utilized VRAM on RTX 4090.</sub>

<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*

