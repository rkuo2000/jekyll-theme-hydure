---
layout: post
title: Vision Language Models
author: [Richard Kuo]
category: [Lecture]
tags: [jekyll, ai]
---

Introduction to VLMs/MLLMs

---
## MLLM - Multimodal Large Language Model
**Paper:** [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549)<br>
![](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/raw/main/images/timeline.jpg)

### [MLLM papers](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

---
## VLM - Vision Language Model
### [Guide to Vision-Language Models (VLMs)](https://encord.com/blog/vision-language-models-guide)
**[LLM in Vision papers](https://github.com/DirtyHarryLYL/LLM-in-Vision)**<br>

### Contrastive Learning
**CLIP architecture**<br>
![](https://images.prismic.io/encord/04311c42-2635-40fb-9187-2847b87224a7_image9.png?auto=compress,format)

---
### PrefixLM
**SimVLM architecture**<br>
![](https://images.prismic.io/encord/80a849e3-8867-4414-bde0-da76df2314de_image3.png?auto=compress,format)

**VirTex architecture**<br>
![](https://images.prismic.io/encord/39a495c5-b563-4253-8722-c58dda94eeb3_image2.png?auto=compress,format)

**Frozen architecture**<br>
![](https://images.prismic.io/encord/65b4b1ac-6af0-41e2-9ac7-e7cdbd67a589_image10.png?auto=compress,format)

**Flamingo architecture**<br>
![](https://images.prismic.io/encord/b94edb70-7caf-4944-9836-2d58dad8b91a_image4.png?auto=compress,format)

---
### Multimodal Fusing with Cross-Attention
**VisualGPT architecture**<br>
![](https://images.prismic.io/encord/90aecd3e-cb7d-4df9-b0df-a64c1a9f9a9d_image6.png?auto=compress,format)

---
### Masked-language Modeling (MLM) & Image-Text Matching (ITM)
**VisualBERT architecture**<br>
![](https://images.prismic.io/encord/9e2fdf0f-24ca-4bf6-9df2-712da43be018_image12.png?auto=compress,format)

---
## MM-LLMs
**Paper:** [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/html/2401.13601v1)<br>
![](https://arxiv.org/html/2401.13601v1/x1.png)

**The general model architecture of MM-LLMs**<br>
![](https://arxiv.org/html/2401.13601v1/x2.png)

**Paper:** [Beyond Human Vision: The Role of Large Vision Language Models in Microscope Image Analysis](https://arxiv.org/abs/2405.00876)<br>

---
### [Next-GPT](https://next-gpt.github.io/)
**Paper:** [Any-to-Any Multimodal Large Language Model](https://arxiv.org/abs/2309.05519)<br>
![](https://next-gpt.github.io/static/images/framework.png)

---
### Ferret
**Paper:** [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://arxiv.org/abs/2310.07704)<br>
**Code:** [https://github.com/apple/ml-ferret](https://github.com/apple/ml-ferret)<br>
![](https://github.com/apple/ml-ferret/raw/main/figs/ferret_fig_diagram_v2.png)

---
### MiniGPT-v2
**Paper:** [MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning](https://arxiv.org/abs/2310.09478)<br>
**Code:** [https://github.com/Vision-CAIR/MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)<br>
![](https://github.com/Vision-CAIR/MiniGPT-4/raw/main/figs/minigpt2_demo.png)

---
### GPT4-V
**Paper:** [Assessing GPT4-V on Structured Reasoning Tasks](https://arxiv.org/abs/2312.11524)<br>

---
### Gemini
**Paper:** [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)<br>
![](https://github.com/rkuo2000/AI-course/blob/main/images/Gemini.png?raw=true)

---
### [PaLM-E](https://palm-e.github.io/)
**Paper:** [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)<br>
**Code:** [https://github.com/kyegomez/PALM-E](https://github.com/kyegomez/PALM-E)<br>
![](https://github.com/kyegomez/PALM-E/raw/main/image6.png)

---
### PaLI-X
**Paper:** [PaLI-X: On Scaling up a Multilingual Vision and Language Model](https://arxiv.org/abs/2305.18565)<br>

---
### Qwen-VL
**model:** [Qwen/Qwen-VL-Chat](https://huggingface.co/Qwen/Qwen-VL-Chat)<br>
**Paper:** [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)<br>
**Code:** [https://github.com/QwenLM/Qwen-VL](https://github.com/QwenLM/Qwen-VL)<br>

---
### Yi-VL-34B
**model:** [01-ai/Yi-VL-34B](https://huggingface.co/01-ai/Yi-VL-34B)<br>

---
### FuYu-8B
**Blog:** [Fuyu-8B: A Multimodal Architecture for AI Agents](https://www.adept.ai/blog/fuyu-8b)<br>
![](https://www.adept.ai/images/blog/fuyu-8b/architecture.png)
![](https://github.com/rkuo2000/AI-course/blob/main/images/VLMs_benchmark.png?raw=true)

---
### [LLaVA](https://llava-vl.github.io/)
**Paper:** [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)<br>
**Paper:** [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)<br>
**Code:** [https://github.com/haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)<br>
![](https://github.com/rkuo2000/GenAI/blob/main/assets/LLaVA_Gradio_Server_UI.png?raw=true)

---
### LLaVA-Med
**Paper:** [LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day](https://arxiv.org/abs/2306.00890)<br>
**Code:** [https://github.com/microsoft/LLaVA-Med](https://github.com/microsoft/LLaVA-Med)<br>

---
### [LLaVA-Plus](https://llava-vl.github.io/llava-plus/)
**Paper:** [LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents](https://arxiv.org/abs/2311.05437)<br>
**Code:** [https://github.com/LLaVA-VL/LLaVA-Plus-Codebase](https://github.com/LLaVA-VL/LLaVA-Plus-Codebase)<br>
![](https://llava-vl.github.io/llava-plus/images/llava-plus-hero.png)
![](https://llava-vl.github.io/llava-plus/images/llava-plus-example.png)

---
### [LLaVA-NeXT](https://llava-vl.github.io/blog/2024-01-30-llava-next/)
LLaVA-NeXT: Improved reasoning, OCR, and world knowledge<br>
Compared with LLaVA-1.5, LLaVA-NeXT has several improvements:<br>
* Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.
* Better visual reasoning and OCR capability with an improved visual instruction tuning data mixture.
* Better visual conversation for more scenarios, covering different applications. Better world knowledge and logical reasoning.
* Efficient deployment and inference with SGLang.

---
### [Florence-2](microsoft/Florence-2-large)
**model:** [microsoft/Florence-2-large](https://huggingface.co/microsoft/Florence-2-large)<br>
**Paper:** [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](https://arxiv.org/abs/2311.06242)<br>
**Blog:** [Florence-2: Advancing Multiple Vision Tasks with a Single VLM Model](https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0)<br>
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*AgN5ojBuiXe1GQyfWkHwRQ.png)

---
### VILA 
**Paper:** [VILA: On Pre-training for Visual Language Models](https://arxiv.org/abs/2312.07533)<br>
**Code:** [https://github.com/Efficient-Large-Model/VILA](https://github.com/Efficient-Large-Model/VILA)<br>
#### VILA on Jetson Orin
<video width="779" height="540" controls><source src="https://github.com/rkuo2000/AI-course/raw/main/images/VILAonJetsonOrin.mp4" type="video/mp4"></video>

---
### [VLFeedback and Silkie](https://vlf-silkie.github.io/)
**Paper:** [Silkie: Preference Distillation for Large Visual Language Models](https://arxiv.org/abs/2312.10665)<br>
**Code:** [https://github.com/vlf-silkie/VLFeedback](https://github.com/vlf-silkie/VLFeedback)<br>

---
### MobileVLM
**Paper:** [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766)<br>
**Code:** [https://github.com/Meituan-AutoML/MobileVLM](https://github.com/Meituan-AutoML/MobileVLM)<br>

---
### [MyVLM](https://snap-research.github.io/MyVLM/)
**Paper:** [MyVLM: Personalizing VLMs for User-Specific Queries](https://arxiv.org/abs/2403.14599)<br>
**Code:** [https://github.com/snap-research/MyVLM](https://github.com/snap-research/MyVLM)<br>
![](https://github.com/snap-research/MyVLM/blob/master/docs/method.jpg?raw=true)
![](https://github.com/snap-research/MyVLM/raw/master/docs/teaser.jpg)

---
### [Reka Core](https://reka-ai)
**Paper:** [Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models](https://publications.reka.ai/reka-core-tech-report.pdf)<br>
![](https://the-decoder.com/wp-content/uploads/2024/04/reka_architecture.png)
![](https://images.squarespace-cdn.com/content/v1/66118bc053ae495c0021e80f/811fc96a-79e1-42d2-95a6-efc0b5e7e57d/reka+core%28blog%29+%283%29.jpg?format=1500w)

---
### InternLM-XComposer
**Code:** [https://github.com/InternLM/InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer)<br>
**InternLM-XComposer2-4KHD** could further understand 4K Resolution images.<br>
![](https://github.com/InternLM/InternLM-XComposer/raw/main/assets/4khd_radar.png)

---
### [Phi-3](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/)
**model:** [microsoft/Phi-3-vision-128k-instruct](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct)<br>
* Phi-3-vision is a **4.2B** parameter multimodal model with language and vision capabilities.
* Phi-3-mini is a 3.8B parameter language model, available in two context lengths (128K and 4K).
* Phi-3-small is a 7B parameter language model, available in two context lengths (128K and 8K).
* Phi-3-medium is a 14B parameter language model, available in two context lengths (128K and 4K).

---
### [MiniCPM-V](https://github.com/OpenBMB/MiniCPM-V)
**model:** [openbmb/MiniCPM-Llama3-V-2_5-int4](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4)<br>
![](https://github.com/OpenBMB/MiniCPM-V/raw/main/assets/MiniCPM-Llama3-V-2.5-peformance.png)

---
### SoM
**Paper:** [Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V](https://arxiv.org/abs/2310.11441)<br>
**Code:** [https://github.com/microsoft/SoM](https://github.com/microsoft/SoM)<br>
![](https://github.com/microsoft/SoM/blob/main/assets/method2_xyz.png?raw=true)

---
### SoM-LLaVA
**Paper:** [List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs](https://arxiv.org/abs/2404.16375)<br>
**Code:** [https://github.com/zzxslp/SoM-LLaVA](https://github.com/zzxslp/SoM-LLaVA)<br>
![](https://github.com/zzxslp/SoM-LLaVA/blob/main/examples/case1.png?raw=true)
![](https://github.com/zzxslp/SoM-LLaVA/blob/main/examples/case2.png?raw=true)

---
### [Gemini-1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)

---
### [Paligemma](https://huggingface.co/blog/paligemma)
**mode:** [google/paligemma-3b-pt-224](https://huggingface.co/google/paligemma-3b-pt-224)<br>
**Paper:** [PaliGemma: A versatile 3B VLM for transfer](https://arxiv.org/abs/2407.07726)<br>
![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/paligemma/paligemma_arch.png)

---
### LongLLaVA
**Blog:** [LongLLaVA: Revolutionizing Multi-Modal AI with Hybrid Architecture](https://blog.gopenai.com/revolutionizing-multi-modal-ai-how-longllava-efficiently-handles-1000-images-with-hybrid-59236bb06609)<br>
**Paper:** [LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture](https://arxiv.org/abs/2409.02889)<br>
![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*F5fk272n2Qsb73l1JZd7WA.jpeg)
**Github:** [https://github.com/FreedomIntelligence/LongLLaVA](https://github.com/FreedomIntelligence/LongLLaVA)<br>

---
### [CogVLM2](https://github.com/THUDM/CogVLM2)
**Paper:** [CogVLM2: Visual Language Models for Image and Video Understanding](https://arxiv.org/abs/2408.16500)<br>
**[Demo](http://cogvlm2-online.cogviewai.cn:7868/)** <br>

---
### Phi-3.5-vision
**model:** [microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)<br>

---
### Pixtral
**model:** [mistralai/Pixtral-12B-2409](https://huggingface.co/mistralai/Pixtral-12B-2409)<br>
**Pixtral 12B** - the first-ever multimodal Mistral model. Apache 2.0.<br>
- New 400M parameter vision encoder trained from scratch
- 12B parameter multimodal decoder based on Mistral Nemo
- Supports variable image sizes and aspect ratios
- Supports multiple images in the long context window of 128k tokens

---
## VLN (Vision-and-Language Navigation)

### [NaVid](https://pku-epic.github.io/NaVid/)
**Paper:** [NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation](https://arxiv.org/abs/2402.15852)<br>
![](https://pku-epic.github.io/NaVid/static/images/method.png)

---
## Audio LLM

### [Qwen-Audio](https://qwen-audio.github.io/Qwen-Audio/)
**Paper:** [Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models](https://arxiv.org/abs/2311.07919)<br>
**Code:** [https://github.com/QwenLM/Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)<br>
![](https://qwen-audio.github.io/Qwen-Audio/static/images/framework.png)
![](https://qwen-audio.github.io/Qwen-Audio/static/audio/radar_new.png)

---
## RELF

### Octopus
**Paper:** [Octopus: Embodied Vision-Language Programmer from Environmental Feedback](https://arxiv.org/abs/2310.08588)<br>
**Code:** [https://github.com/dongyh20/Octopus](https://github.com/dongyh20/Octopus)<br>
<iframe width="942" height="530" src="https://www.youtube.com/embed/tmSNw2XonxI" title="Introducing Project Octopus" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---
### [VLM-R1](https://github.com/om-ai-lab/VLM-R1)
VLM-R1: A stable and generalizable R1-style Large Vision-Language Model<br>
![](https://github.com/om-ai-lab/VLM-R1/raw/main/assets/performance.png)

<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*


